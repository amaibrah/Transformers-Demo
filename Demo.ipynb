{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install torch\n",
    "#!pip3 install pyPDF2\n",
    "#!pip3 install wget\n",
    "#!pip3 install sentencepiece\n",
    "#!pip3 install protobuf==3.20.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 24% |  7% |\n",
      "|  1 | 24% |  6% |\n",
      "|  2 | 23% |  7% |\n",
      "|  3 | 19% |  6% |\n"
     ]
    }
   ],
   "source": [
    "from GPUtil import showUtilization as gpu_usage\n",
    "gpu_usage()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import needed libraries\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import pipeline, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import requests\n",
    "import io\n",
    "from PyPDF2 import PdfFileReader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of summarization models is to create a summary using new phrases built by interpreting and understanding a text. This leads to a more \"human-like\" sounding summary as opposed to an extractive summary that only extracts important information verbatim from a text. This goal is met by using transformers that use encoder and decoder layers, similar to a Seq2Seq model (sequence to sequence). \n",
    "\n",
    "Each encoder layer consists of a self-attention and feed forward layer. An input, in this case a sentence, is fed into the self-attention layer where the encoder looks at other words while encoding a specific word. This is repeated for every word in a sentence before being passed on to the decoder portion of the transformer. The decoder consists of the same layers as an encoder, but has an extra layer, the attention layer, that allows the decoder to focus on relevant parts of the sentence.\n",
    "\n",
    "The attention layers of the encoder accesses words in the text while the attention layers of the decoder accesses the words positioned before the given word input to create meaning.\n",
    "\n",
    "\n",
    "In this case, BART (Bidirectional and Auto-Regressive Transformer) is used. BART uses Seq2Seq with a bidirectional encoder and a left-to-right decoder similar to GPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class summarization:\n",
    "    def __init__(self, demo=False):\n",
    "        if not demo:\n",
    "            #allow for inout of other text\n",
    "            self.sequence= input('What would you like to summarize?')\n",
    "        else:\n",
    "            self.sequence= \"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-story building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.\"\n",
    "\n",
    "    def predict(self):\n",
    "        #choose model that performs summarization\n",
    "        checkpoint= 'facebook/bart-large-cnn'\n",
    "    \n",
    "        #the model has already been pretrained and finetuned to data from cnn()\n",
    "        model= BartForConditionalGeneration.from_pretrained(checkpoint)\n",
    "        tokenizer= BartTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "        #apply the tokenizer to the text\n",
    "        #padding is used to ensure that the shorter sentences have the same length as the longsest sequence or the max length accepted by the model\n",
    "        inputs= tokenizer(self.sequence, padding= True, return_tensors=\"pt\")\n",
    "\n",
    "        #pass encoded tokens along with specific parameters\n",
    "        outputs= model.generate(inputs['input_ids'], max_length=150, min_length= 50, length_penalty= 2.0, num_beams= 10, early_stopping= True)\n",
    "        #decode the generated summary into text\n",
    "        summary= tokenizer.decode(outputs[0], skip_special_tokens= True)\n",
    "\n",
    "        print('Original text: ')\n",
    "        print(self.sequence)\n",
    "        print('')\n",
    "        print('Summary: ')\n",
    "        print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: \n",
      "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-story building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.\n",
      "\n",
      "Summary: \n",
      "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-story building, and the tallest structure in Paris. Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.\n"
     ]
    }
   ],
   "source": [
    "summarization(demo=True).predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization model finetuned on conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the formatting of text messages/transcripts, the pretrained model does nto summarize the conversation in the best way. This model was fine-tuned and trained on the samsum dataset found on HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conversation_summarization:\n",
    "    def __init__(self, demo=False):\n",
    "        #allow for input\n",
    "        if not demo:\n",
    "            self.sequence= input('What would you like to summarize?')\n",
    "        else:\n",
    "            self.sequence= \"\"\"Hannah: Hey, do you have Betty's number?\n",
    "Amanda: Lemme check\n",
    "Amanda: Sorry, can't find it.\n",
    "Amanda: Ask Larry\n",
    "Amanda: He called her last time we were at the park together\n",
    "Hannah: I don't know him well\n",
    "Amanda: Don't be shy, he's very nice\n",
    "Hannah: If you say so..\n",
    "Hannah: I'd rather you texted him\n",
    "Amanda: Just text him ðŸ™‚\n",
    "Hannah: Urgh.. Alright\n",
    "Hannah: Bye\n",
    "Amanda: Bye bye\"\"\"\n",
    "    \n",
    "    def predict(self):\n",
    "        #choose model that performs coversation summarization, trained on samsum data\n",
    "        checkpoint= 'amaibrah/bart-large-samsum'\n",
    "    \n",
    "        model= AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "        tokenizer= AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "        #apply the tokenizer to the text\n",
    "        #padding is used to ensure that the shorter sentences have the same length as the longsest sequence or the max length accepted by the model\n",
    "        #PyTorch is being used, so the tensors should be set as 'pt' in this case\n",
    "        inputs= tokenizer(self.sequence, padding= True, return_tensors=\"pt\")\n",
    "        #create a summary no longer than 150 words and no less than 15\n",
    "        outputs= model.generate(inputs['input_ids'], max_length=150, min_length= 15, length_penalty= 2.0, num_beams= 4, early_stopping= True)\n",
    "        summary= tokenizer.decode(outputs[0], skip_special_tokens= True)\n",
    "\n",
    "        print('Original text: ')\n",
    "        print(self.sequence)\n",
    "        print('')\n",
    "        print('Summary: ')\n",
    "        print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e23742e75e444a98a7348f2fa2ac5e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.66k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6bdabd572754c2bb755094609b48452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dfebebfe10f489cbcf510903a411650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c1a860db8154b0e98fd3afc7a94d968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98f9651354d94b0f8ed5f43a0653d131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3548a6041f5c4a7d94d06c8d5b18334b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffdf2a7b337142d6b5d2a7bf90f57586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: \n",
      "Hannah: Hey, do you have Betty's number?\n",
      "Amanda: Lemme check\n",
      "Amanda: Sorry, can't find it.\n",
      "Amanda: Ask Larry\n",
      "Amanda: He called her last time we were at the park together\n",
      "Hannah: I don't know him well\n",
      "Amanda: Don't be shy, he's very nice\n",
      "Hannah: If you say so..\n",
      "Hannah: I'd rather you texted him\n",
      "Amanda: Just text him ðŸ™‚\n",
      "Hannah: Urgh.. Alright\n",
      "Hannah: Bye\n",
      "Amanda: Bye bye\n",
      "\n",
      "Summary: \n",
      "Amanda can't find Betty's number. Larry called Betty the last time they were at the park together. Hannah doesn't know Larry very well. Amanda will text him.\n"
     ]
    }
   ],
   "source": [
    "conversation_summarization(demo=True).predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization model finetuned on long documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BART-large-CNN model has some limitations when it comes to summarizing long documents. This model was fine-tuned on the launch/gov_report dataset from HuggingFace. It contains long reports and their associated summaries written by goverment research agencies. This model does take some time to run depending on the length of the document.\n",
    "\n",
    "When using the class below with demo set to true, the model will automatically summarize chapter 2 of and open source political science textbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class long_doc_summarization:\n",
    "    def __init__(self, demo=False):\n",
    "        if not demo:\n",
    "            #in case someone would like to summarize a specific document\n",
    "            self.url= input('Please enter the url to your PDF document:')\n",
    "            #first page in pdf to be summarized\n",
    "            self.start= input(\"What page would you like to start at?\")\n",
    "            #last page in pdf to be summarized\n",
    "            self.end= input(\"What page would you like to end at?\")\n",
    "            \n",
    "            #extract the pdf's content from the webpage\n",
    "            r= requests.get(self.url)\n",
    "            f= io.BytesIO(r.content)\n",
    "            reader= PdfFileReader(f, strict= False)\n",
    "\n",
    "            #extract content from each page chosen\n",
    "            self.sequence= []\n",
    "            for i in range(self.start, self.end+1):\n",
    "                contents= reader.getPage(i).extractText()\n",
    "                self.sequence.append(contents)\n",
    "\n",
    "        else:\n",
    "            #url to open access pdf file of a political science textbook\n",
    "            url= \"https://web.ung.edu/media/university-press/public-policy.pdf?t=1661449833017\"\n",
    "            r= requests.get(url)\n",
    "            f= io.BytesIO(r.content)\n",
    "            reader= PdfFileReader(f, strict=False)\n",
    "            #Chapter 2 of textbook\n",
    "            start= 21\n",
    "            end= 40\n",
    "            self.sequence= []\n",
    "            for i in range(start, end+1):\n",
    "                contents= reader.getPage(i).extractText()\n",
    "                self.sequence.append(contents)\n",
    "\n",
    "    def predict(self):\n",
    "        #choose model that performs coversation summarization, trained on samsum data\n",
    "        checkpoint= 'amaibrah/long_doc_summarizer'\n",
    "    \n",
    "        model= AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "        tokenizer= AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "        inputs= tokenizer(self.sequence, padding= True, return_tensors=\"pt\")\n",
    "        #return summary that is no more than 1000 words and no less than 250\n",
    "        outputs= model.generate(inputs['input_ids'], max_length=1000, min_length= 250, length_penalty= 2.0, num_beams= 10, early_stopping= True)\n",
    "        summary= tokenizer.decode(outputs[0], skip_special_tokens= True)\n",
    "\n",
    "        print('Summary: ')\n",
    "        print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2d012e1f551445fa733ce758659abfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.66k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f57cf256d3af4016a00ec9f97996f21c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cb0442d8b2647f58a4cb92b83b15327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1ab9121159e40429412ac12171663a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d1e2e8291a2419cab8ebcea5f498e85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d68a0b9a16648de815832bc2b0631a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa074525987140e4aeed4f272d1ce54f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: \n",
      "The Articles of Confederation were introduced in 1776 as an attempt to create a new, permanent government in the American colonies. By 1781, the Articles had been ratified and had officially become the law of the land. Fearing a return to the oppressive policies associated with a strong unitary government, Americans were reluctant to give too much power to their new central government. The framers opted for a confederate government where policy making power was placed in the hands of local (state) governments. The U.S. Constitution, written in 1787, created the new federal government and established the foundation for federal government policy making. The problem the framers of the Constitution faced was power. How much political power should be given to the government, and where should it be placed? Not enough political power in the central government could lead to anarchy and the tyranny of the majority, that is, when a majority controls a representative. Stop and Think: Why did the Articles of. Confederation make it difficult for a centralized government to make policy?    What the American democratic experiment needed was a revolutionary new form of government that would combine the advantages of unitary and confederates governments and increase the. policy making abilities of the federal government.\n"
     ]
    }
   ],
   "source": [
    "long_doc_summarization(demo=True).predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-shot Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal of zero-shot classification is to be able to classify text without using any labeled data and without seeing labelled text. Zero-shot classification models can be used on text of a different domain that is was not initially trained on. This makes these types of models best for generalized topics overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'I love traveling',\n",
       " 'labels': ['travel', 'entertainment', 'technology', 'dancing', 'cooking'],\n",
       " 'scores': [0.9668570756912231,\n",
       "  0.02837473899126053,\n",
       "  0.002383037470281124,\n",
       "  0.0014391953591257334,\n",
       "  0.0009460083092562854]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using HuggingFace's pipeline function to show model's performance\n",
    "classifier= pipeline(task=\"zero-shot-classification\", device=0, model=\"joeddav/xlm-roberta-large-xnli\")\n",
    "\n",
    "input_sequence = \"I love traveling\"\n",
    "label_candidate = ['travel', 'cooking', 'entertainment', 'dancing', 'technology']\n",
    "classifier(input_sequence, label_candidate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above shows the label that is most likely to be related out of the list of labels. This is a single-label version of zero-shot classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero-shot classification can also be used for multi-label classification where it tests the probablity of each label being similar to the statement or input. Each probability score is calculated independently instead of together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model was pretrained on multiple languages as listed below. The statement and labels do not have to be in the same language for this model!\n",
    "\n",
    "The languages include: English, French, Spanish, German, Greek, Bulgarian, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, Hindi, Swahili, and Urdu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class zero_shot_model:\n",
    "    def __init__(self, demo=False):\n",
    "        #can accept input\n",
    "        if not demo:\n",
    "            self.statement= input('What would you like to classify?')\n",
    "            #the input can be a list of words separated by commas\n",
    "            self.labels= [str(x) for x in input(\"Please list the labels you would like to use for this classification. Use a comma to separate each word: \").split(', ')]\n",
    "\n",
    "        else:\n",
    "        #automatic inputs if demo is selected\n",
    "            self.statement= \"I love traveling\"\n",
    "            self.labels= ['travel', 'cooking', 'entertainment', 'dancing', 'technology']\n",
    "\n",
    "    def predict(self):\n",
    "        #select pretrained model\n",
    "        checkpoint= 'joeddav/xlm-roberta-large-xnli'\n",
    "\n",
    "        model= AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "        tokenizer= AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "        #create empty dictionary that will be addended using the loop below\n",
    "        output={}\n",
    "        for label in self.labels:\n",
    "            x= tokenizer.encode(self.statement, label, return_tensors='pt', truncation_strategy='only_first')\n",
    "            logits= model(x)[0]\n",
    "            #compute probability of how related each label is to the statement\n",
    "            entail_contradiction_logits= logits[:, [0,2]]\n",
    "            probs= entail_contradiction_logits.softmax(dim=1)\n",
    "            output[label]= float(probs[:,1])\n",
    "\n",
    "        #create function that returns labels with probability in descending order\n",
    "        def sort(d, reverse=False):\n",
    "            return dict(sorted(d.items(), key= lambda x: x[1], reverse=reverse))\n",
    "\n",
    "        print(self.statement)\n",
    "        print('')\n",
    "        print(sort(output, True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love traveling\n",
      "\n",
      "{'travel': 0.9993754029273987, 'entertainment': 0.922992467880249, 'cooking': 0.33314982056617737, 'dancing': 0.1990966647863388, 'technology': 0.043315738439559937}\n"
     ]
    }
   ],
   "source": [
    "zero_shot_model(demo=True).predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('demoEnv': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "abab9fb3ef906d878718f3c698ddd49c743193d3b013b964c504f3fdd96ee333"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
