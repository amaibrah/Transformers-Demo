{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used to summarize large amounts of text. Can be done in multiple languages. Most models were initially trained on either articles from BBC or CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BART allows for summaries to be built using new phrases and terms while keeping the overall meaning (abstractive summarization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import needed libraries\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoConfig\n",
    "\n",
    "#choose model that performs summarization\n",
    "checkpoint= 'facebook/bart-large-cnn'\n",
    "\n",
    "#the model has already been pretrained and finetuned to data from xsum ()\n",
    "model= AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "tokenizer= AutoTokenizer.from_pretrained(checkpoint)\n",
    "config= AutoConfig.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use article found on CNN to test out summarization\n",
    "sequence= \"\"\"Airlines, airports and the federal government are bracing for aviation infrastructure to take a major blow from Hurricane Ian. Cancellations and closures are already piling up across the Florida peninsula.\n",
    "The storm is forecast to make landfall Wednesday afternoon on Florida's west coast as a major hurricane.\n",
    "Tampa International Airport, where officials are preparing for a major impact, suspended operations at 5 p.m. ET Tuesday.\n",
    "\n",
    "The Tampa airport said there will be no departing flights through Thursday.\n",
    "'We will share a reopening date and time when it is determined,' the airport said on Twitter Wednesday. The airport typically handles 450 flights daily.\n",
    "Miami International Airport was still open midday Wednesday, according to a notice on the airport's website, although some flights had been delayed or canceled.\n",
    "Operations ceased at 10:30 am ET Wednesday at Orlando International Airport. The airport sees nearly 130,000 passengers daily, according to its website.\n",
    "The terminal at St. Pete-Clearwater International Airport closed at 1 p.m. Tuesday \"due to mandatory evacuation orders from Pinellas County and remain closed until the evacuation order is lifted,\" according to the verified tweet from the airport.\n",
    "Sarasota Bradenton International Airport closed at 8 p.m. Tuesday night.\n",
    "Florida airports lead in US cancellations\n",
    "By midday Wednesday, FlightAware data showed more than 2,100 US flight cancellations nationwide on Wednesday. About 1,700 Thursday flights had already been canceled.\n",
    "Orlando, Miami and Tampa airports were the top three trouble spots, with cancellations also mounting at Fort Lauderdale International Airport and Southwest Florida International Airport in Fort Myers.\n",
    "Effects could ripple through the southeastern United States with Atlanta and Charlotte already seeing cancellations.\n",
    "Airlines canceling flights\n",
    "American Airlines, which operates about 250 daily departures out of Miami, its fourth-largest hub, had canceled 583 flights by midday Wednesday, including mainline and regional service.\n",
    "American customers traveling through 20 airports in the hurricane's path can rebook flights without change fees. The airline has also added \"reduced, last-minute fares for cities that will be impacted\" in hopes of helping people who are trying to \"evacuate via air.\"\n",
    "United Airlines is starting to shutter operations on the Atlantic Coast of Florida in anticipation of Hurricane Ian's path after it makes landfall.\n",
    "By Wednesday afternoon, United says it will halt departures from West Palm Beach, Miami and Fort Lauderdale airports. United will not operate from Jacksonville starting on Thursday.\n",
    "United said on Wednesday that it had proactively canceled 345 flights since Tuesday, swapping some outbound flights with larger airplanes to help customers who were evacuating from the storm's path.\n",
    "United and Southwest Airlines also suspended operations at the Fort Myers and Sarasota airports.\n",
    "United also canceled all Tuesday and Wednesday flights to and from Key West and canceled some flights out of Orlando 'as to minimize crew layovers.'\n",
    "By midday Wednesday, Southwest Airlines had canceled more than 500 US flights, according to FlightAware data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orlando, Miami and Tampa airports were the top three trouble spots, with cancellations also mounting at Fort Lauderdale International Airport and Southwest Florida International Airport in Fort Myers. United Airlines is starting to shutter operations on the Atlantic Coast of Florida in anticipation of Hurricane Ian's path after it makes landfall.\n"
     ]
    }
   ],
   "source": [
    "inputs= tokenizer(sequence, padding= True, return_tensors=\"pt\")\n",
    "#change max length, min length, length penalty, num_beams\n",
    "outputs= model.generate(inputs['input_ids'], max_length=100, min_length= 40, length_penalty= 2.0, num_beams= 10, early_stopping= True)\n",
    "#print summary of article above\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens= True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model on conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a dataset that has text conversations and their summaries to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset samsum (/home/aibrah/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "268dceb9d1064fbbae2a0d835aa735a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/aibrah/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-12af2190291d7181.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70d7bf26dcca45e49f4ec5f1f7fd6a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/aibrah/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-c5acd4d37aa86677.arrow\n"
     ]
    }
   ],
   "source": [
    "data= load_dataset(\"samsum\")\n",
    "checkpoint= 'facebook/bart-large-cnn'\n",
    "    \n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "tokenizer= AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "device= torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [doc for doc in examples[\"dialogue\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length= 1000, truncation=True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length= 150, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_data= data.map(preprocess_function, batched=True)\n",
    "\n",
    "\n",
    "#customize the training by specifying attributes\n",
    "batch_size= 4\n",
    "model_name= checkpoint\n",
    "args= Seq2SeqTrainingArguments(\n",
    "    output_dir= 'txt_summarization',\n",
    "    evaluation_strategy= 'epoch',\n",
    "    learning_rate= 2e-5,\n",
    "    per_device_train_batch_size= batch_size,\n",
    "    per_device_eval_batch_size= batch_size,\n",
    "    weight_decay= 0.01,\n",
    "    save_total_limit= 2,\n",
    "    num_train_epochs= 3,\n",
    "    predict_with_generate= True,\n",
    "    fp16= True)\n",
    "    \n",
    "data_collator= DataCollatorForSeq2Seq(tokenizer, model= model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: dialogue, summary, id. If dialogue, summary, id are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 14732\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 11049\n",
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11049' max='11049' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11049/11049 39:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.359400</td>\n",
       "      <td>1.460210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.013400</td>\n",
       "      <td>1.415231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.760600</td>\n",
       "      <td>1.493712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to txt_summarization/checkpoint-500\n",
      "Configuration saved in txt_summarization/checkpoint-500/config.json\n",
      "Model weights saved in txt_summarization/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in txt_summarization/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in txt_summarization/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to txt_summarization/checkpoint-1000\n",
      "Configuration saved in txt_summarization/checkpoint-1000/config.json\n",
      "Model weights saved in txt_summarization/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in txt_summarization/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in txt_summarization/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to txt_summarization/checkpoint-1500\n",
      "Configuration saved in txt_summarization/checkpoint-1500/config.json\n",
      "Model weights saved in txt_summarization/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in txt_summarization/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in txt_summarization/checkpoint-1500/special_tokens_map.json\n",
      "Deleting older checkpoint [txt_summarization/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to txt_summarization/checkpoint-2000\n",
      "Configuration saved in txt_summarization/checkpoint-2000/config.json\n",
      "Model weights saved in txt_summarization/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in txt_summarization/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in txt_summarization/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [txt_summarization/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to txt_summarization/checkpoint-2500\n",
      "Configuration saved in txt_summarization/checkpoint-2500/config.json\n",
      "Model weights saved in txt_summarization/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in txt_summarization/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in txt_summarization/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [txt_summarization/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to txt_summarization/checkpoint-3000\n",
      "Configuration saved in txt_summarization/checkpoint-3000/config.json\n",
      "Model weights saved in txt_summarization/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in txt_summarization/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in txt_summarization/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [txt_summarization/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to txt_summarization/checkpoint-3500\n",
      "Configuration saved in txt_summarization/checkpoint-3500/config.json\n",
      "Model weights saved in txt_summarization/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in txt_summarization/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in txt_summarization/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [txt_summarization/checkpoint-2500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: dialogue, summary, id. If dialogue, summary, id are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 818\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to txt_summarization/checkpoint-4000\n",
      "Configuration saved in txt_summarization/checkpoint-4000/config.json\n",
      "Model weights saved in txt_summarization/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in txt_summarization/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in txt_summarization/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [txt_summarization/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to txt_summarization/checkpoint-4500\n",
      "Configuration saved in txt_summarization/checkpoint-4500/config.json\n",
      "Model weights saved in txt_summarization/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in txt_summarization/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in txt_summarization/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [txt_summarization/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to txt_summarization/checkpoint-5000\n",
      "Configuration saved in txt_summarization/checkpoint-5000/config.json\n",
      "Model weights saved in txt_summarization/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in txt_summarization/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in txt_summarization/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [txt_summarization/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to txt_summarization/checkpoint-5500\n",
      "Configuration saved in txt_summarization/checkpoint-5500/config.json\n",
      "Model weights saved in txt_summarization/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in txt_summarization/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in txt_summarization/checkpoint-5500/special_tokens_map.json\n",
      "Deleting older checkpoint [txt_summarization/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to txt_summarization/checkpoint-6000\n",
      "Configuration saved in txt_summarization/checkpoint-6000/config.json\n",
      "Model weights saved in txt_summarization/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in txt_summarization/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in txt_summarization/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [txt_summarization/checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to txt_summarization/checkpoint-6500\n",
      "Configuration saved in txt_summarization/checkpoint-6500/config.json\n",
      "Model weights saved in txt_summarization/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in txt_summarization/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in txt_summarization/checkpoint-6500/special_tokens_map.json\n",
      "Deleting older checkpoint [txt_summarization/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to txt_summarization/checkpoint-7000\n",
      "Configuration saved in txt_summarization/checkpoint-7000/config.json\n",
      "Model weights saved in txt_summarization/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in txt_summarization/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in txt_summarization/checkpoint-7000/special_tokens_map.json\n",
      "Deleting older checkpoint [txt_summarization/checkpoint-6000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: dialogue, summary, id. If dialogue, summary, id are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 818\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to txt_summarization/checkpoint-7500\n",
      "Configuration saved in txt_summarization/checkpoint-7500/config.json\n",
      "Model weights saved in txt_summarization/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in txt_summarization/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in txt_summarization/checkpoint-7500/special_tokens_map.json\n",
      "Deleting older checkpoint [txt_summarization/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to txt_summarization/checkpoint-8000\n",
      "Configuration saved in txt_summarization/checkpoint-8000/config.json\n",
      "Model weights saved in txt_summarization/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in txt_summarization/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in txt_summarization/checkpoint-8000/special_tokens_map.json\n",
      "Deleting older checkpoint [txt_summarization/checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to txt_summarization/checkpoint-8500\n",
      "Configuration saved in txt_summarization/checkpoint-8500/config.json\n",
      "Model weights saved in txt_summarization/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in txt_summarization/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in txt_summarization/checkpoint-8500/special_tokens_map.json\n",
      "Deleting older checkpoint [txt_summarization/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to txt_summarization/checkpoint-9000\n",
      "Configuration saved in txt_summarization/checkpoint-9000/config.json\n",
      "Model weights saved in txt_summarization/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in txt_summarization/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in txt_summarization/checkpoint-9000/special_tokens_map.json\n",
      "Deleting older checkpoint [txt_summarization/checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to txt_summarization/checkpoint-9500\n",
      "Configuration saved in txt_summarization/checkpoint-9500/config.json\n",
      "Model weights saved in txt_summarization/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in txt_summarization/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in txt_summarization/checkpoint-9500/special_tokens_map.json\n",
      "Deleting older checkpoint [txt_summarization/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to txt_summarization/checkpoint-10000\n",
      "Configuration saved in txt_summarization/checkpoint-10000/config.json\n",
      "Model weights saved in txt_summarization/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in txt_summarization/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in txt_summarization/checkpoint-10000/special_tokens_map.json\n",
      "Deleting older checkpoint [txt_summarization/checkpoint-9000] due to args.save_total_limit\n",
      "Saving model checkpoint to txt_summarization/checkpoint-10500\n",
      "Configuration saved in txt_summarization/checkpoint-10500/config.json\n",
      "Model weights saved in txt_summarization/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in txt_summarization/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in txt_summarization/checkpoint-10500/special_tokens_map.json\n",
      "Deleting older checkpoint [txt_summarization/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to txt_summarization/checkpoint-11000\n",
      "Configuration saved in txt_summarization/checkpoint-11000/config.json\n",
      "Model weights saved in txt_summarization/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in txt_summarization/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in txt_summarization/checkpoint-11000/special_tokens_map.json\n",
      "Deleting older checkpoint [txt_summarization/checkpoint-10000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: dialogue, summary, id. If dialogue, summary, id are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 818\n",
      "  Batch size = 4\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=11049, training_loss=1.0619153366206253, metrics={'train_runtime': 2391.3146, 'train_samples_per_second': 18.482, 'train_steps_per_second': 4.62, 'total_flos': 2.651417773149389e+16, 'train_loss': 1.0619153366206253, 'epoch': 3.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./conversation_summarizer_model\n",
      "Configuration saved in ./conversation_summarizer_model/config.json\n",
      "Model weights saved in ./conversation_summarizer_model/pytorch_model.bin\n",
      "tokenizer config file saved in ./conversation_summarizer_model/tokenizer_config.json\n",
      "Special tokens file saved in ./conversation_summarizer_model/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "#save new model\n",
    "trainer.save_model(\"./conversation_summarizer_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./conversation_summarizer_model/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"./conversation_summarizer_model\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"force_bos_token_to_be_generated\": true,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 56,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.22.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "loading weights file ./conversation_summarizer_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at ./conversation_summarizer_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hannah is looking for Betty's number but Amanda can't find it. Larry called Betty the last time they were at the park together. Hannah doesn't know him well but Amanda thinks he's nice.\n"
     ]
    }
   ],
   "source": [
    "#test out new model\n",
    "checkpoint= './conversation_summarizer_model'\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "tokenizer= AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sequence= input('What would you like to summarize?')\n",
    "\n",
    "inputs= tokenizer(sequence, padding= True, return_tensors=\"pt\")\n",
    "outputs= model.generate(inputs['input_ids'], max_length=100, min_length= 15, length_penalty= 2.0, num_beams= 10, early_stopping= True)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens= True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 142, but you input_length is only 121. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=60)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': \"Amanda can't find Betty's number. Larry called Betty the last time Amanda and Hannah were at the park together. Hannah doesn't know Larry very well, so Amanda will text him to ask him about Betty's contact details. Hannah and Amanda will talk to him later.\"}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text= \"\"\"Hannah: Hey, do you have Betty's number?\n",
    "Amanda: Lemme check\n",
    "Amanda: Sorry, can't find it.\n",
    "Amanda: Ask Larry\n",
    "Amanda: He called her last time we were at the park together\n",
    "Hannah: I don't know him well\n",
    "Amanda: Don't be shy, he's very nice\n",
    "Hannah: If you say so..\n",
    "Hannah: I'd rather you texted him\n",
    "Amanda: Just text him 🙂\n",
    "Hannah: Urgh.. Alright\n",
    "Hannah: Bye\n",
    "Amanda: Bye bye\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('demoEnv': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "abab9fb3ef906d878718f3c698ddd49c743193d3b013b964c504f3fdd96ee333"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
